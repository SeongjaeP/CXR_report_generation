{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOaF7z7/5nr0DnO/wkMLXvj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"161pikXIEAb9"},"outputs":[],"source":["import torch\n","from torch import nn\n","from transformers import AutoConfig, AutoModel, SwinModel, ViTModel\n","\n","\n","class HuggingfaceImageEncoder(nn.Module):\n","    def __init__(\n","        self,\n","        name: str = \"google/vit-base-patch16-224\",\n","        pretrained: bool = True,\n","        gradient_checkpointing: bool = False,\n","        cache_dir: str = \"~/.cache/huggingface/hub\",\n","        model_type: str = \"vit\",\n","        local_files_only: bool = False,\n","    ):\n","        super().__init__()\n","        self.model_type = model_type\n","        if pretrained:\n","            if self.model_type == \"swin\":\n","                self.image_encoder = SwinModel.from_pretrained(name)\n","            else:\n","                self.image_encoder = AutoModel.from_pretrained(\n","                    name, add_pooling_layer=False, cache_dir=cache_dir, local_files_only=local_files_only\n","                )\n","        else:\n","            # initializing with a config file does not load the weights associated with the model\n","            model_config = AutoConfig.from_pretrained(name, cache_dir=cache_dir, local_files_only=local_files_only)\n","            if type(model_config).__name__ == \"ViTConfig\":\n","                self.image_encoder = ViTModel(model_config, add_pooling_layer=False)\n","            else:\n","                # TODO: add vision models if needed\n","                raise NotImplementedError(f\"Not support training from scratch : {type(model_config).__name__}\")\n","\n","        if gradient_checkpointing and self.image_encoder.supports_gradient_checkpointing:\n","            self.image_encoder.gradient_checkpointing_enable()\n","\n","        self.out_dim = self.image_encoder.config.hidden_size\n","\n","    def forward(self, image):\n","        if self.model_type == \"vit\":\n","            output = self.image_encoder(pixel_values=image, interpolate_pos_encoding=True)\n","        elif self.model_type == \"swin\":\n","            output = self.image_encoder(pixel_values=image)\n","        return output[\"last_hidden_state\"]  # (batch, seq_len, hidden_size)"]},{"cell_type":"code","source":["!pip install --upgrade transformers"],"metadata":{"id":"7n-l3BhSMwsC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm -rf /root/.cache/huggingface/"],"metadata":{"id":"D40BZUu5M-Ie"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import ViTModel\n","\n","model = ViTModel.from_pretrained('google/vit-base-patch16-224', cache_dir='/tmp/huggingface_hub')"],"metadata":{"id":"SaQdY2cGKm9u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","notebook_login()"],"metadata":{"id":"eecN5JjIFLcw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","from torchvision import transforms\n","\n","# 이미지를 로드합니다.\n","image = Image.open('/content/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.jpg')\n","\n","# 이미지 전처리를 위한 변환을 정의합니다.\n","preprocess = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=3),\n","    transforms.Resize((224, 224)),  # 모델 입력 사이즈에 맞게 이미지 크기를 조정합니다.\n","    transforms.ToTensor(),          # 이미지를 텐서로 변환합니다.\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # 정규화를 진행합니다.\n","])\n","\n","# 이미지를 전처리합니다.\n","image_tensor = preprocess(image).unsqueeze(0)  # 배치 차원을 추가합니다.\n","\n","# 모델 인스턴스를 생성합니다.\n","encoder = HuggingfaceImageEncoder()\n","\n","# 이미지를 모델에 넣어 인코딩합니다.\n","with torch.no_grad():  # 그래디언트 계산을 하지 않도록 설정합니다.\n","    encoded_images = encoder(image_tensor)"],"metadata":{"id":"goSLv_zBEFFS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm -rf /root/.cache/huggingface/"],"metadata":{"id":"CvHYQDB1ES7x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import ViTImageProcessor, ViTForImageClassification\n","from PIL import Image\n","import requests\n","\n","url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n","image = Image.open(requests.get(url, stream=True).raw)\n","\n","processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n","model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n","\n","inputs = processor(images=image, return_tensors=\"pt\")\n","outputs = model(**inputs)\n","logits = outputs.logits\n","# model predicts one of the 1000 ImageNet classes\n","predicted_class_idx = logits.argmax(-1).item()\n","print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"],"metadata":{"id":"Ezi636mkJgQI"},"execution_count":null,"outputs":[]}]}